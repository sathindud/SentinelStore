# ğŸ—‚ï¸ Distributed File Storage System with ZooKeeper

This project implements a **fault-tolerant distributed file storage system** using **ZooKeeper for leader election** and a **consensus algorithm** for maintaining data consistency across nodes.

Each node can act as a leader or follower.
- The **leader** handles client requests and coordinates replication.
- **Followers** synchronize with the leader and store replicas.

Files are uploaded to a majority of nodes, ensuring fault tolerance and consistency.  
Downloads use a majority-vote consensus to determine the correct file version.

---

## ğŸ§© Features

- Leader election using **ZooKeeper**.
- Logical clockâ€“based ordering for concurrent requests.
- Heartbeat-based failure detection.
- File replication to **majority nodes**.
- Majority-vote consensus during download.
- Fault-tolerant recovery via logs.
- Simple client interface for upload/download.

---

## ğŸ§± Prerequisites

1. **Java 17+**
2. **Apache ZooKeeper**
    - Download from: [https://zookeeper.apache.org/releases.html](https://zookeeper.apache.org/releases.html)
3. **IntelliJ IDEA (Recommended)**
    - Open the project folder and run each node from IntelliJ with runtime arguments.

---

## âš™ï¸ Setup Instructions

### Step 1: Start ZooKeeper
After installing ZooKeeper,

first edit or create zoo.cfg file in the conf folder

```bash
tickTime=2000
initLimit=10
syncLimit=5
dataDir=zookeeper/data
clientPort=2181
````

then start the server:

```bash
zkServer.sh start
```

## Step 2: Run Server Nodes
You need to run 5 server nodes (each in a separate IntelliJ run instance or terminal).

1. In IntelliJ:
2. Open the Node.java file.
3. Click on Run â†’ Edit Configurations...

Under Program Arguments, add:

``` php-template
<nodeName> <portNumber>
```
Example:

```yaml
node1 5001
node2 5002
node3 5003
node4 5004
node5 5005
```
Each node will:

- Connect to ZooKeeper.
- Elect a leader automatically.
- Print its role in the console.

Example output:

```csharp
[ZooKeeper] Leader elected: node1
Node node1 started as Leader on port 5001
Node node2 started as Follower on port 5002
```
...

## Step 3: Run Client
Once the nodes are running, you can run the Client.java to upload and download files.

### Upload a File
Create a new Client Instance woith the following arguments


``` php-template
upload <file path>
```
Example:
Their is a sample.txt file in the root folder

``` nginx
upload sample.txt
```

âœ… The leader will:

- Assign a unique File ID.
- Replicate it to a majority of followers.
- Display which followers received the file.

Example output:

``` rust
Leader elected: Node1 at localhost:5001
Client got response: File ID: 0fd7bd07-4cf2-4008-a8a6-f1f6c3b93b40 successfully replicated to majority (2/2 nodes):
```

### Download a File.
To download, run:

``` php-template
download <fileId>
```

Example:

``` nginx
download 9d4a7c5b-6f8a-4b32-bc9a-1b1d4a4238d5
```

The leader will:
 - Query all followers.
 - Perform majority-vote consensus on the file content.
 - Return the agreed-upon version.

Example output:

``` sql
 Hello from the distributed system!
```

If no majority is found:

``` nginx
File content received:
NO_FILE_FOUND - No majority consensus (max votes: 1/2)
``` 

## Step 4: Node Logs
Each node keeps a local log of uploads with timestamps:

``` lua
log.txt
```

Example log entry:

``` makefile
1733344942552:9d4a7c5b-6f8a-4b32-bc9a-1b1d4a4238d5
```

ğŸ“‚ Project Structure
```bash
Copy code
ğŸ“¦ distributed-storage
 â”£ ğŸ“œ Node.java          # Main server 
 â”£ ğŸ“œ Client.java          # Client File
 â”£ ğŸ“‚ node_Node1_files/           # Node folder 1
    â”£ ğŸ“„ 0fd7bd07-4cf2-4008-a8a6-f1f6c3b93b40.txt #Stored File
    â”£ ğŸ“„ log.txt # Log file
 â”£ ğŸ“‚ node_Node2_files/           # Node folder 2
 â”£ ğŸ“‚ node_Node3_files/           # Node folder 3
 â”£ ğŸ“‚ node_Node4_files/           # Node folder 4
 â”— ğŸ“‚ node_Node5_files/           # Node folder 5
```

## ğŸ§  Design Summary
- Consensus: Achieved via majority voting on replicas.
- Replication: Majority-based active replication by leader.
- Fault Tolerance: Uses heartbeats and ZooKeeper re-election.
- Concurrency: Handled using logical timestamps in logs.

## ğŸ‘¨â€ğŸ’» **Authors**

âœ¨ Developed by the **AlertEye Distributed Systems Team** âœ¨

- ğŸ§‘â€ğŸ’» **T.S.D. De Zoysa** â€” [`it23669062@my.sliit.lk`](mailto:it23669062@my.sliit.lk)
- ğŸ‘©â€ğŸ’» **D.T.D. Wijethunga** â€” [`it23614062@my.sliit.lk`](mailto:it23614062@my.sliit.lk)
- ğŸ‘¨â€ğŸ’» **Wickramaarachchi D S** â€” [`it23736450@my.sliit.lk`](mailto:it23736450@my.sliit.lk)
- ğŸ‘¨â€ğŸ’» **L.G.A.I. De Silva** â€” [`it23632028@my.sliit.lk`](mailto:it23632028@my.sliit.lk)


for the Distributed Systems Project.

